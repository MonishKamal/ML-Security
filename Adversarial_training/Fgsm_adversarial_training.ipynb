{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfaba4a7",
   "metadata": {},
   "source": [
    "## FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe864751-4978-4ed7-8bf0-57f8a225f037",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvjitlink-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvjitlink-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /home/sasidharp/.local/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.5.147 from https://files.pythonhosted.org/packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12==12.4.127 from https://files.pythonhosted.org/packages/ff/ff/847841bacfbefc97a00036e0fce5a0f086b640756dc38caea5e1bb002655/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-nvjitlink-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nvidia-curand-cu12 nvidia-nvjitlink-cu12-12.4.127\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision in /home/sasidharp/.local/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: cleverhans in /home/sasidharp/.local/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: numpy in /home/sasidharp/.local/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.5.1 in /home/sasidharp/.local/lib/python3.11/site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (2023.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (11.2.1.3)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchvision)\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.5.147 from https://files.pythonhosted.org/packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/sasidharp/.local/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: nose in /home/sasidharp/.local/lib/python3.11/site-packages (from cleverhans) (1.3.7)\n",
      "Requirement already satisfied: pycodestyle in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from cleverhans) (2.10.0)\n",
      "Requirement already satisfied: scipy in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from cleverhans) (1.11.1)\n",
      "Requirement already satisfied: matplotlib in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from cleverhans) (3.7.2)\n",
      "Requirement already satisfied: mnist in /home/sasidharp/.local/lib/python3.11/site-packages (from cleverhans) (0.2.2)\n",
      "Requirement already satisfied: tensorflow-probability in /home/sasidharp/.local/lib/python3.11/site-packages (from cleverhans) (0.24.0)\n",
      "Requirement already satisfied: joblib in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from cleverhans) (1.2.0)\n",
      "Requirement already satisfied: easydict in /home/sasidharp/.local/lib/python3.11/site-packages (from cleverhans) (1.13)\n",
      "Requirement already satisfied: absl-py in /home/sasidharp/.local/lib/python3.11/site-packages (from cleverhans) (2.1.0)\n",
      "Requirement already satisfied: six in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from cleverhans) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from matplotlib->cleverhans) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from matplotlib->cleverhans) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from matplotlib->cleverhans) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from matplotlib->cleverhans) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from matplotlib->cleverhans) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from matplotlib->cleverhans) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from matplotlib->cleverhans) (2.8.2)\n",
      "Requirement already satisfied: decorator in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from tensorflow-probability->cleverhans) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from tensorflow-probability->cleverhans) (2.2.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /home/sasidharp/.local/lib/python3.11/site-packages (from tensorflow-probability->cleverhans) (0.6.0)\n",
      "Requirement already satisfied: dm-tree in /home/sasidharp/.local/lib/python3.11/site-packages (from tensorflow-probability->cleverhans) (0.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /cm/shared/apps/anaconda3/3.11/lib/python3.11/site-packages (from jinja2->torch==2.5.1->torchvision) (2.1.1)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: nvidia-curand-cu12\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nvidia-curand-cu12\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-curand-cu12 (/home/sasidharp/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision cleverhans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2f75fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_loader = DataLoader(\n",
    "    datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform),\n",
    "    batch_size=1000, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1119b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.model = models.resnet18(weights=None)  # Use weights=None instead of pretrained=False\n",
    "        # Modify the first conv layer to accept 1-channel input\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Remove maxpool layer to reduce spatial downsampling\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        # Modify the fully connected layer to output 10 classes\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f1cd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FGSM attack implementation\n",
    "def fgsm_attack(model, data, target, epsilon):\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = data.grad.data\n",
    "    perturbed_data = data + epsilon * data_grad.sign()\n",
    "    return perturbed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6569d1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adversarial training with accuracy metrics\n",
    "def train(model, device, train_loader, optimizer, epsilon, epoch):\n",
    "    model.train()\n",
    "    total_loss, total_adv_loss = 0, 0\n",
    "    correct, correct_adv = 0, 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Standard forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy for clean data\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        # Generate adversarial examples using FGSM\n",
    "        adv_data = fgsm_attack(model, data, target, epsilon)\n",
    "        adv_output = model(adv_data)\n",
    "        adv_loss = F.cross_entropy(adv_output, target)\n",
    "        total_adv_loss += adv_loss.item()\n",
    "\n",
    "        # Compute accuracy for adversarial data\n",
    "        adv_pred = adv_output.argmax(dim=1, keepdim=True)\n",
    "        correct_adv += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "\n",
    "        # Combined loss\n",
    "        total_batch_loss = (loss + adv_loss) / 2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += target.size(0)\n",
    "\n",
    "        # Log information for each batch\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '\n",
    "                  f'Loss: {loss.item():.4f} | Adv Loss: {adv_loss.item():.4f} | '\n",
    "                  f'Acc: {100. * correct / total:.2f}% | Adv Acc: {100. * correct_adv / total:.2f}%')\n",
    "\n",
    "    # Log average loss and accuracy for epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    avg_acc = 100. * correct / total\n",
    "    avg_adv_acc = 100. * correct_adv / total\n",
    "    print(f'==> Epoch: {epoch+1} | Avg Loss: {avg_loss:.4f} | Avg Adv Loss: {avg_adv_loss:.4f} | '\n",
    "          f'Avg Acc: {avg_acc:.2f}% | Avg Adv Acc: {avg_adv_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd4797dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet18().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828dd1ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a625d96e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/60000] Loss: 2.4609 | Adv Loss: 2.9688 | Acc: 9.38% | Adv Acc: 1.56%\n",
      "Epoch: 1 [6400/60000] Loss: 0.7250 | Adv Loss: 1.0568 | Acc: 74.26% | Adv Acc: 58.23%\n",
      "Epoch: 1 [12800/60000] Loss: 0.4496 | Adv Loss: 0.7671 | Acc: 78.56% | Adv Acc: 63.99%\n",
      "Epoch: 1 [19200/60000] Loss: 0.2926 | Adv Loss: 0.5025 | Acc: 80.69% | Adv Acc: 66.81%\n",
      "Epoch: 1 [25600/60000] Loss: 0.4391 | Adv Loss: 0.6936 | Acc: 81.82% | Adv Acc: 68.45%\n",
      "Epoch: 1 [32000/60000] Loss: 0.3118 | Adv Loss: 0.5052 | Acc: 82.82% | Adv Acc: 69.76%\n",
      "Epoch: 1 [38400/60000] Loss: 0.2165 | Adv Loss: 0.4870 | Acc: 83.52% | Adv Acc: 70.80%\n",
      "Epoch: 1 [44800/60000] Loss: 0.3683 | Adv Loss: 0.6605 | Acc: 84.11% | Adv Acc: 71.81%\n",
      "Epoch: 1 [51200/60000] Loss: 0.4115 | Adv Loss: 0.9137 | Acc: 84.59% | Adv Acc: 72.40%\n",
      "Epoch: 1 [57600/60000] Loss: 0.4035 | Adv Loss: 0.7646 | Acc: 84.88% | Adv Acc: 72.90%\n",
      "==> Epoch: 1 | Avg Loss: 0.3994 | Avg Adv Loss: 0.6955 | Avg Acc: 85.00% | Avg Adv Acc: 73.07%\n",
      "Epoch 1/5 completed\n",
      "\n",
      "Epoch: 2 [0/60000] Loss: 0.4143 | Adv Loss: 0.7884 | Acc: 85.94% | Adv Acc: 73.44%\n",
      "Epoch: 2 [6400/60000] Loss: 0.1527 | Adv Loss: 0.3700 | Acc: 89.12% | Adv Acc: 78.82%\n",
      "Epoch: 2 [12800/60000] Loss: 0.4189 | Adv Loss: 0.7362 | Acc: 89.06% | Adv Acc: 78.87%\n",
      "Epoch: 2 [19200/60000] Loss: 0.2468 | Adv Loss: 0.4865 | Acc: 89.11% | Adv Acc: 78.54%\n",
      "Epoch: 2 [25600/60000] Loss: 0.2711 | Adv Loss: 0.4822 | Acc: 89.33% | Adv Acc: 78.81%\n",
      "Epoch: 2 [32000/60000] Loss: 0.2701 | Adv Loss: 0.4897 | Acc: 89.33% | Adv Acc: 78.83%\n",
      "Epoch: 2 [38400/60000] Loss: 0.2505 | Adv Loss: 0.4589 | Acc: 89.32% | Adv Acc: 78.90%\n",
      "Epoch: 2 [44800/60000] Loss: 0.2742 | Adv Loss: 0.5445 | Acc: 89.32% | Adv Acc: 78.99%\n",
      "Epoch: 2 [51200/60000] Loss: 0.2580 | Adv Loss: 0.5168 | Acc: 89.40% | Adv Acc: 79.06%\n",
      "Epoch: 2 [57600/60000] Loss: 0.3202 | Adv Loss: 0.5714 | Acc: 89.55% | Adv Acc: 79.19%\n",
      "==> Epoch: 2 | Avg Loss: 0.2779 | Avg Adv Loss: 0.5268 | Avg Acc: 89.57% | Avg Adv Acc: 79.25%\n",
      "Epoch 2/5 completed\n",
      "\n",
      "Epoch: 3 [0/60000] Loss: 0.3562 | Adv Loss: 0.5878 | Acc: 87.50% | Adv Acc: 78.12%\n",
      "Epoch: 3 [6400/60000] Loss: 0.1966 | Adv Loss: 0.4632 | Acc: 90.83% | Adv Acc: 79.69%\n",
      "Epoch: 3 [12800/60000] Loss: 0.2754 | Adv Loss: 0.4992 | Acc: 90.89% | Adv Acc: 80.20%\n",
      "Epoch: 3 [19200/60000] Loss: 0.3357 | Adv Loss: 0.6803 | Acc: 90.94% | Adv Acc: 80.28%\n",
      "Epoch: 3 [25600/60000] Loss: 0.3012 | Adv Loss: 0.5675 | Acc: 90.77% | Adv Acc: 80.28%\n",
      "Epoch: 3 [32000/60000] Loss: 0.1423 | Adv Loss: 0.3197 | Acc: 90.85% | Adv Acc: 80.64%\n",
      "Epoch: 3 [38400/60000] Loss: 0.2512 | Adv Loss: 0.4565 | Acc: 90.74% | Adv Acc: 80.68%\n",
      "Epoch: 3 [44800/60000] Loss: 0.1698 | Adv Loss: 0.3363 | Acc: 90.90% | Adv Acc: 80.81%\n",
      "Epoch: 3 [51200/60000] Loss: 0.2829 | Adv Loss: 0.4849 | Acc: 90.93% | Adv Acc: 80.85%\n",
      "Epoch: 3 [57600/60000] Loss: 0.1380 | Adv Loss: 0.2989 | Acc: 90.94% | Adv Acc: 80.91%\n",
      "==> Epoch: 3 | Avg Loss: 0.2388 | Avg Adv Loss: 0.4729 | Avg Acc: 90.94% | Avg Adv Acc: 80.97%\n",
      "Epoch 3/5 completed\n",
      "\n",
      "Epoch: 4 [0/60000] Loss: 0.1928 | Adv Loss: 0.4586 | Acc: 93.75% | Adv Acc: 81.25%\n",
      "Epoch: 4 [6400/60000] Loss: 0.1543 | Adv Loss: 0.3530 | Acc: 91.46% | Adv Acc: 81.42%\n",
      "Epoch: 4 [12800/60000] Loss: 0.2117 | Adv Loss: 0.5419 | Acc: 91.77% | Adv Acc: 81.49%\n",
      "Epoch: 4 [19200/60000] Loss: 0.2551 | Adv Loss: 0.4667 | Acc: 91.86% | Adv Acc: 81.91%\n",
      "Epoch: 4 [25600/60000] Loss: 0.2936 | Adv Loss: 0.5132 | Acc: 91.75% | Adv Acc: 82.01%\n",
      "Epoch: 4 [32000/60000] Loss: 0.1765 | Adv Loss: 0.3266 | Acc: 91.84% | Adv Acc: 82.03%\n",
      "Epoch: 4 [38400/60000] Loss: 0.2428 | Adv Loss: 0.4602 | Acc: 91.87% | Adv Acc: 82.13%\n",
      "Epoch: 4 [44800/60000] Loss: 0.3357 | Adv Loss: 0.6018 | Acc: 91.87% | Adv Acc: 82.11%\n",
      "Epoch: 4 [51200/60000] Loss: 0.1863 | Adv Loss: 0.4789 | Acc: 91.83% | Adv Acc: 82.02%\n",
      "Epoch: 4 [57600/60000] Loss: 0.1824 | Adv Loss: 0.3745 | Acc: 91.80% | Adv Acc: 82.09%\n",
      "==> Epoch: 4 | Avg Loss: 0.2172 | Avg Adv Loss: 0.4464 | Avg Acc: 91.76% | Avg Adv Acc: 82.09%\n",
      "Epoch 4/5 completed\n",
      "\n",
      "Epoch: 5 [0/60000] Loss: 0.1929 | Adv Loss: 0.3999 | Acc: 90.62% | Adv Acc: 79.69%\n",
      "Epoch: 5 [6400/60000] Loss: 0.1358 | Adv Loss: 0.2851 | Acc: 92.05% | Adv Acc: 82.66%\n",
      "Epoch: 5 [12800/60000] Loss: 0.1177 | Adv Loss: 0.3658 | Acc: 92.44% | Adv Acc: 83.08%\n",
      "Epoch: 5 [19200/60000] Loss: 0.1998 | Adv Loss: 0.5324 | Acc: 92.56% | Adv Acc: 83.15%\n",
      "Epoch: 5 [25600/60000] Loss: 0.0807 | Adv Loss: 0.2194 | Acc: 92.81% | Adv Acc: 83.34%\n",
      "Epoch: 5 [32000/60000] Loss: 0.1526 | Adv Loss: 0.3171 | Acc: 92.76% | Adv Acc: 83.29%\n",
      "Epoch: 5 [38400/60000] Loss: 0.1708 | Adv Loss: 0.3969 | Acc: 92.69% | Adv Acc: 83.17%\n",
      "Epoch: 5 [44800/60000] Loss: 0.1380 | Adv Loss: 0.2822 | Acc: 92.71% | Adv Acc: 83.23%\n",
      "Epoch: 5 [51200/60000] Loss: 0.1295 | Adv Loss: 0.3141 | Acc: 92.77% | Adv Acc: 83.32%\n",
      "Epoch: 5 [57600/60000] Loss: 0.2363 | Adv Loss: 0.4747 | Acc: 92.76% | Adv Acc: 83.43%\n",
      "==> Epoch: 5 | Avg Loss: 0.1923 | Avg Adv Loss: 0.4074 | Avg Acc: 92.75% | Avg Adv Acc: 83.39%\n",
      "Epoch 5/5 completed\n",
      "\n",
      "Adversarial training completed.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "epsilon = 0.1  \n",
    "for epoch in range(epochs):\n",
    "    train(model, device, train_loader, optimizer, epsilon, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed\\n\")\n",
    "\n",
    "print(\"Adversarial training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e67c6",
   "metadata": {},
   "source": [
    "Sp, The decrease in both average losses over epochs suggests the model is improving on both clean and adversarial examples, learning to classify both correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04081312",
   "metadata": {},
   "source": [
    "## ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a852495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_loader = DataLoader(\n",
    "    datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform),\n",
    "    batch_size=1000, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83207048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a ResNet50 model adapted for grayscale images and 10 classes\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = models.resnet50(weights=None)  # Use weights=None instead of pretrained=False\n",
    "        # Modify the first conv layer to accept 1-channel input\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Remove maxpool layer to reduce spatial downsampling\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        # Modify the fully connected layer to output 10 classes\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9784e491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FGSM attack implementation\n",
    "def fgsm_attack(model, data, target, epsilon):\n",
    "    data.requires_grad = True\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = data.grad.data\n",
    "    perturbed_data = data + epsilon * data_grad.sign()\n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a030071e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adversarial training with accuracy metrics\n",
    "def train(model, device, train_loader, optimizer, epsilon, epoch):\n",
    "    model.train()\n",
    "    total_loss, total_adv_loss = 0, 0\n",
    "    correct, correct_adv = 0, 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Standard forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy for clean data\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        # Generate adversarial examples using FGSM\n",
    "        adv_data = fgsm_attack(model, data, target, epsilon)\n",
    "        adv_output = model(adv_data)\n",
    "        adv_loss = F.cross_entropy(adv_output, target)\n",
    "        total_adv_loss += adv_loss.item()\n",
    "\n",
    "        # Compute accuracy for adversarial data\n",
    "        adv_pred = adv_output.argmax(dim=1, keepdim=True)\n",
    "        correct_adv += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "\n",
    "        # Combined loss\n",
    "        total_batch_loss = (loss + adv_loss) / 2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += target.size(0)\n",
    "\n",
    "        # Log information for each batch\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '\n",
    "                  f'Loss: {loss.item():.4f} | Adv Loss: {adv_loss.item():.4f} | '\n",
    "                  f'Acc: {100. * correct / total:.2f}% | Adv Acc: {100. * correct_adv / total:.2f}%')\n",
    "\n",
    "    # Log average loss and accuracy for epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    avg_acc = 100. * correct / total\n",
    "    avg_adv_acc = 100. * correct_adv / total\n",
    "    print(f'==> Epoch: {epoch+1} | Avg Loss: {avg_loss:.4f} | Avg Adv Loss: {avg_adv_loss:.4f} | '\n",
    "          f'Avg Acc: {avg_acc:.2f}% | Avg Adv Acc: {avg_adv_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e0e2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet50().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c38715",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/60000] Loss: 2.4655 | Adv Loss: 2.4578 | Acc: 10.94% | Adv Acc: 12.50%\n",
      "Epoch: 1 [6400/60000] Loss: 0.7652 | Adv Loss: 1.1625 | Acc: 67.76% | Adv Acc: 55.00%\n",
      "Epoch: 1 [12800/60000] Loss: 0.6373 | Adv Loss: 1.0035 | Acc: 74.68% | Adv Acc: 62.06%\n",
      "Epoch: 1 [19200/60000] Loss: 0.3459 | Adv Loss: 0.5804 | Acc: 77.71% | Adv Acc: 65.14%\n",
      "Epoch: 1 [25600/60000] Loss: 0.4209 | Adv Loss: 0.7779 | Acc: 79.58% | Adv Acc: 67.18%\n",
      "Epoch: 1 [32000/60000] Loss: 0.4691 | Adv Loss: 0.8537 | Acc: 80.68% | Adv Acc: 68.46%\n",
      "Epoch: 1 [38400/60000] Loss: 0.2549 | Adv Loss: 0.4828 | Acc: 81.68% | Adv Acc: 69.66%\n",
      "Epoch: 1 [44800/60000] Loss: 0.3454 | Adv Loss: 0.6307 | Acc: 82.31% | Adv Acc: 70.39%\n",
      "Epoch: 1 [51200/60000] Loss: 0.2989 | Adv Loss: 0.5535 | Acc: 82.90% | Adv Acc: 71.20%\n",
      "Epoch: 1 [57600/60000] Loss: 0.4240 | Adv Loss: 0.7741 | Acc: 83.47% | Adv Acc: 71.74%\n",
      "==> Epoch: 1 | Avg Loss: 0.4470 | Avg Adv Loss: 0.7546 | Avg Acc: 83.61% | Avg Adv Acc: 71.88%\n",
      "Epoch 1/5 completed\n",
      "\n",
      "Epoch: 2 [0/60000] Loss: 0.3754 | Adv Loss: 0.6804 | Acc: 79.69% | Adv Acc: 76.56%\n",
      "Epoch: 2 [6400/60000] Loss: 0.1942 | Adv Loss: 0.4192 | Acc: 88.51% | Adv Acc: 77.63%\n",
      "Epoch: 2 [12800/60000] Loss: 0.1596 | Adv Loss: 0.3889 | Acc: 88.59% | Adv Acc: 77.65%\n",
      "Epoch: 2 [19200/60000] Loss: 0.2966 | Adv Loss: 0.5951 | Acc: 88.65% | Adv Acc: 77.91%\n",
      "Epoch: 2 [25600/60000] Loss: 0.3403 | Adv Loss: 0.6738 | Acc: 88.86% | Adv Acc: 78.04%\n",
      "Epoch: 2 [32000/60000] Loss: 0.8264 | Adv Loss: 1.3023 | Acc: 88.19% | Adv Acc: 77.16%\n",
      "Epoch: 2 [38400/60000] Loss: 0.1296 | Adv Loss: 0.2685 | Acc: 88.10% | Adv Acc: 77.23%\n",
      "Epoch: 2 [44800/60000] Loss: 0.2887 | Adv Loss: 0.5893 | Acc: 88.30% | Adv Acc: 77.51%\n",
      "Epoch: 2 [51200/60000] Loss: 0.2123 | Adv Loss: 0.3514 | Acc: 88.46% | Adv Acc: 77.76%\n",
      "Epoch: 2 [57600/60000] Loss: 0.2573 | Adv Loss: 0.5100 | Acc: 88.58% | Adv Acc: 77.99%\n",
      "==> Epoch: 2 | Avg Loss: 0.3112 | Avg Adv Loss: 0.5806 | Avg Acc: 88.60% | Avg Adv Acc: 78.00%\n",
      "Epoch 2/5 completed\n",
      "\n",
      "Epoch: 3 [0/60000] Loss: 0.2175 | Adv Loss: 0.4068 | Acc: 90.62% | Adv Acc: 84.38%\n",
      "Epoch: 3 [6400/60000] Loss: 0.2392 | Adv Loss: 0.4519 | Acc: 90.52% | Adv Acc: 81.08%\n",
      "Epoch: 3 [12800/60000] Loss: 0.2933 | Adv Loss: 0.5853 | Acc: 90.62% | Adv Acc: 81.38%\n",
      "Epoch: 3 [19200/60000] Loss: 0.2009 | Adv Loss: 0.4115 | Acc: 90.50% | Adv Acc: 81.21%\n",
      "Epoch: 3 [25600/60000] Loss: 0.4333 | Adv Loss: 0.7275 | Acc: 90.43% | Adv Acc: 81.21%\n",
      "Epoch: 3 [32000/60000] Loss: 0.2568 | Adv Loss: 0.4923 | Acc: 90.23% | Adv Acc: 80.51%\n",
      "Epoch: 3 [38400/60000] Loss: 0.2066 | Adv Loss: 0.4168 | Acc: 90.25% | Adv Acc: 80.31%\n",
      "Epoch: 3 [44800/60000] Loss: 0.1397 | Adv Loss: 0.4196 | Acc: 90.17% | Adv Acc: 80.22%\n",
      "Epoch: 3 [51200/60000] Loss: 0.1562 | Adv Loss: 0.3696 | Acc: 90.09% | Adv Acc: 80.01%\n",
      "Epoch: 3 [57600/60000] Loss: 0.3027 | Adv Loss: 0.5247 | Acc: 90.08% | Adv Acc: 80.09%\n",
      "==> Epoch: 3 | Avg Loss: 0.2641 | Avg Adv Loss: 0.5075 | Avg Acc: 90.13% | Avg Adv Acc: 80.17%\n",
      "Epoch 3/5 completed\n",
      "\n",
      "Epoch: 4 [0/60000] Loss: 0.1503 | Adv Loss: 0.3602 | Acc: 95.31% | Adv Acc: 81.25%\n",
      "Epoch: 4 [6400/60000] Loss: 0.2360 | Adv Loss: 0.4767 | Acc: 91.94% | Adv Acc: 83.15%\n",
      "Epoch: 4 [12800/60000] Loss: 0.2587 | Adv Loss: 0.5000 | Acc: 91.74% | Adv Acc: 83.58%\n",
      "Epoch: 4 [19200/60000] Loss: 0.1717 | Adv Loss: 0.5343 | Acc: 91.88% | Adv Acc: 84.32%\n",
      "Epoch: 4 [25600/60000] Loss: 0.2356 | Adv Loss: 0.4372 | Acc: 91.76% | Adv Acc: 84.36%\n",
      "Epoch: 4 [32000/60000] Loss: 0.1650 | Adv Loss: 0.3160 | Acc: 91.63% | Adv Acc: 85.05%\n",
      "Epoch: 4 [38400/60000] Loss: 0.3569 | Adv Loss: 0.4430 | Acc: 91.68% | Adv Acc: 85.08%\n",
      "Epoch: 4 [44800/60000] Loss: 0.2080 | Adv Loss: 0.1527 | Acc: 91.68% | Adv Acc: 85.92%\n",
      "Epoch: 4 [51200/60000] Loss: 0.3082 | Adv Loss: 0.1482 | Acc: 91.62% | Adv Acc: 86.12%\n",
      "Epoch: 4 [57600/60000] Loss: 0.2413 | Adv Loss: 0.1468 | Acc: 91.74% | Adv Acc: 86.91%\n",
      "==> Epoch: 4 | Avg Loss: 0.2223 | Avg Adv Loss: 0.3433 | Avg Acc: 91.78% | Avg Adv Acc: 87.26%\n",
      "Epoch 4/5 completed\n",
      "\n",
      "Epoch: 5 [0/60000] Loss: 0.2448 | Adv Loss: 0.1542 | Acc: 90.62% | Adv Acc: 95.31%\n",
      "Epoch: 5 [6400/60000] Loss: 0.1072 | Adv Loss: 0.0868 | Acc: 93.04% | Adv Acc: 93.39%\n",
      "Epoch: 5 [12800/60000] Loss: 0.2085 | Adv Loss: 0.2556 | Acc: 92.84% | Adv Acc: 93.37%\n",
      "Epoch: 5 [19200/60000] Loss: 0.0985 | Adv Loss: 0.1795 | Acc: 92.83% | Adv Acc: 93.50%\n",
      "Epoch: 5 [25600/60000] Loss: 0.1039 | Adv Loss: 0.1350 | Acc: 92.69% | Adv Acc: 93.51%\n",
      "Epoch: 5 [32000/60000] Loss: 0.3151 | Adv Loss: 0.2845 | Acc: 92.69% | Adv Acc: 93.64%\n",
      "Epoch: 5 [38400/60000] Loss: 0.3247 | Adv Loss: 0.1297 | Acc: 92.74% | Adv Acc: 94.17%\n",
      "Epoch: 5 [44800/60000] Loss: 0.1862 | Adv Loss: 0.1303 | Acc: 92.73% | Adv Acc: 94.49%\n",
      "Epoch: 5 [51200/60000] Loss: 0.1661 | Adv Loss: 0.8948 | Acc: 92.79% | Adv Acc: 94.51%\n",
      "Epoch: 5 [57600/60000] Loss: 0.2892 | Adv Loss: 0.1535 | Acc: 92.84% | Adv Acc: 94.74%\n",
      "==> Epoch: 5 | Avg Loss: 0.1935 | Avg Adv Loss: 0.1588 | Avg Acc: 92.83% | Avg Adv Acc: 94.85%\n",
      "Epoch 5/5 completed\n",
      "\n",
      "Adversarial training completed.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "epsilon = 0.1  \n",
    "for epoch in range(epochs):\n",
    "    train(model, device, train_loader, optimizer, epsilon, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed\\n\")\n",
    "\n",
    "print(\"Adversarial training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a6cdb-5f4d-452d-9746-960ba6e3b847",
   "metadata": {
    "tags": []
   },
   "source": [
    "Avg Loss:\n",
    "\n",
    "This is the average loss on clean (non-adversarial) images across the entire epoch. Lower values indicate that the model’s predictions are closer to the true labels.\n",
    "Over the epochs, Avg Loss decreases significantly (from 0.4470 in Epoch 1 to 0.1935 in Epoch 5), indicating that the model is learning well on the clean data.\n",
    "Avg Adv Loss:\n",
    "\n",
    "This is the average loss on adversarial (perturbed) images. Lower values here suggest that the model is becoming more robust to adversarial attacks.\n",
    "The sharp decline in Avg Adv Loss (from 0.7546 in Epoch 1 to 0.1588 in Epoch 5) indicates that the model is learning to correctly classify images even when small, intentional perturbations are added.\n",
    "Avg Acc (Accuracy on clean data):\n",
    "\n",
    "This is the average accuracy on clean images across the epoch. It improves from 83.61% in Epoch 1 to 92.83% by Epoch 5, showing that the model is generalizing better on clean images.\n",
    "Avg Adv Acc (Accuracy on adversarial data):\n",
    "\n",
    "This is the average accuracy on adversarial images, and it’s a key metric for evaluating robustness.\n",
    "Avg Adv Acc starts lower (71.88%) because adversarial examples are challenging. However, it improves significantly to 94.85% by Epoch 5, indicating that the model is now robust against adversarial attacks, almost matching its accuracy on clean images.\n",
    "Overall Interpretation\n",
    "These results show that the model is learning both the original task of classifying clean images and gaining resilience to adversarial perturbations. By the final epoch, Avg Adv Acc is comparable to Avg Acc, meaning the model has learned to defend well against adversarial attacks, achieving high accuracy on both clean and perturbed images. This adversarial training approach has effectively enhanced the model’s robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d22dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07eb7346-488f-40ec-b695-d1ce92a72c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load FashionMNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_loader = DataLoader(\n",
    "    datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform),\n",
    "    batch_size=1000, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9b4888-9bd0-453e-938f-ad7c390cbf60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a ResNet50 model adapted for grayscale images and 10 classes\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = models.resnet50(weights=None)  # Use weights=None instead of pretrained=False\n",
    "        # Modify the first conv layer to accept 1-channel input\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Remove maxpool layer to reduce spatial downsampling\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        # Modify the fully connected layer to output 10 classes\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "# Normalize function\n",
    "def normalize(tensor):\n",
    "    return (tensor - 0.5) / 0.5\n",
    "\n",
    "# Adversarial training with CleverHans for FGSM attack\n",
    "def train(model, device, train_loader, optimizer, epsilon, epoch):\n",
    "    model.train()\n",
    "    total_loss, total_adv_loss = 0, 0\n",
    "    correct, correct_adv = 0, 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Standard forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy for clean data\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        # Generate adversarial examples using CleverHans FGSM without clipping\n",
    "        adv_data = fast_gradient_method(\n",
    "            model_fn=model,\n",
    "            x=data,\n",
    "            eps=epsilon,\n",
    "            norm=np.inf,\n",
    "            clip_min=None,  # Disable clipping to mimic the custom function\n",
    "            clip_max=None,\n",
    "            y=target,\n",
    "            targeted=False\n",
    "        )\n",
    "\n",
    "        # If needed, normalize adv_data to match model's expected input\n",
    "        adv_data = normalize(adv_data)\n",
    "\n",
    "        # Get the model's prediction on the adversarial example\n",
    "        with torch.no_grad():\n",
    "            adv_output = model(normalize(adv_data))  # Normalize the adversarial example before prediction\n",
    "            _, adv_pred_label = torch.max(adv_output, 1)\n",
    "        \n",
    "        adv_loss = F.cross_entropy(adv_output, target)\n",
    "        total_adv_loss += adv_loss.item()\n",
    "\n",
    "        # Compute accuracy for adversarial data\n",
    "        adv_pred = adv_output.argmax(dim=1, keepdim=True)\n",
    "        correct_adv += adv_pred.eq(target.view_as(adv_pred)).sum().item()\n",
    "\n",
    "        # Combined loss\n",
    "        total_batch_loss = (loss + adv_loss) / 2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += target.size(0)\n",
    "\n",
    "        # Log information for each batch\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '\n",
    "                  f'Loss: {loss.item():.4f} | Adv Loss: {adv_loss.item():.4f} | '\n",
    "                  f'Acc: {100. * correct / total:.2f}% | Adv Acc: {100. * correct_adv / total:.2f}%')\n",
    "\n",
    "    # Log average loss and accuracy for epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(train_loader)\n",
    "    avg_acc = 100. * correct / total\n",
    "    avg_adv_acc = 100. * correct_adv / total\n",
    "    print(f'==> Epoch: {epoch+1} | Avg Loss: {avg_loss:.4f} | Avg Adv Loss: {avg_adv_loss:.4f} | '\n",
    "          f'Avg Acc: {avg_acc:.2f}% | Avg Adv Acc: {avg_adv_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cc77c2-7abc-4e08-89c3-7f825fe279a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch: 1 [0/60000] Loss: 2.3952 | Adv Loss: 2.4109 | Acc: 18.75% | Adv Acc: 18.75%\n",
      "Epoch: 1 [6400/60000] Loss: 0.5175 | Adv Loss: 1.1748 | Acc: 67.76% | Adv Acc: 49.60%\n",
      "Epoch: 1 [12800/60000] Loss: 0.5394 | Adv Loss: 1.4170 | Acc: 74.30% | Adv Acc: 54.31%\n",
      "Epoch: 1 [19200/60000] Loss: 0.4681 | Adv Loss: 1.7106 | Acc: 77.56% | Adv Acc: 55.34%\n",
      "Epoch: 1 [25600/60000] Loss: 0.5933 | Adv Loss: 2.1269 | Acc: 79.73% | Adv Acc: 55.10%\n",
      "Epoch: 1 [32000/60000] Loss: 0.3069 | Adv Loss: 1.4107 | Acc: 81.17% | Adv Acc: 55.05%\n",
      "Epoch: 1 [38400/60000] Loss: 0.2571 | Adv Loss: 1.8583 | Acc: 82.22% | Adv Acc: 54.82%\n",
      "Epoch: 1 [44800/60000] Loss: 0.4777 | Adv Loss: 2.1086 | Acc: 83.11% | Adv Acc: 54.69%\n",
      "Epoch: 1 [51200/60000] Loss: 0.2876 | Adv Loss: 2.8580 | Acc: 83.89% | Adv Acc: 54.78%\n",
      "Epoch: 1 [57600/60000] Loss: 0.3013 | Adv Loss: 2.1303 | Acc: 84.54% | Adv Acc: 54.70%\n",
      "==> Epoch: 1 | Avg Loss: 0.4300 | Avg Adv Loss: 1.7482 | Avg Acc: 84.77% | Avg Adv Acc: 54.77%\n",
      "Epoch 1/5 completed\n",
      "\n",
      "Epoch: 2 [0/60000] Loss: 0.4268 | Adv Loss: 1.8219 | Acc: 85.94% | Adv Acc: 53.12%\n",
      "Epoch: 2 [6400/60000] Loss: 0.2246 | Adv Loss: 2.0020 | Acc: 89.94% | Adv Acc: 55.52%\n",
      "Epoch: 2 [12800/60000] Loss: 0.2797 | Adv Loss: 2.3331 | Acc: 90.15% | Adv Acc: 55.65%\n",
      "Epoch: 2 [19200/60000] Loss: 0.5390 | Adv Loss: 3.0099 | Acc: 90.08% | Adv Acc: 55.01%\n",
      "Epoch: 2 [25600/60000] Loss: 0.1969 | Adv Loss: 1.8788 | Acc: 90.18% | Adv Acc: 55.12%\n",
      "Epoch: 2 [32000/60000] Loss: 0.2117 | Adv Loss: 2.0214 | Acc: 89.90% | Adv Acc: 55.13%\n",
      "Epoch: 2 [38400/60000] Loss: 0.1952 | Adv Loss: 1.7868 | Acc: 89.74% | Adv Acc: 54.55%\n",
      "Epoch: 2 [44800/60000] Loss: 0.2669 | Adv Loss: 2.0140 | Acc: 89.84% | Adv Acc: 54.21%\n",
      "Epoch: 2 [51200/60000] Loss: 0.2843 | Adv Loss: 3.0808 | Acc: 90.02% | Adv Acc: 54.35%\n",
      "Epoch: 2 [57600/60000] Loss: 0.3747 | Adv Loss: 2.3631 | Acc: 90.03% | Adv Acc: 54.08%\n",
      "==> Epoch: 2 | Avg Loss: 0.2778 | Avg Adv Loss: 2.2427 | Avg Acc: 89.99% | Avg Adv Acc: 54.01%\n",
      "Epoch 2/5 completed\n",
      "\n",
      "Epoch: 3 [0/60000] Loss: 0.2267 | Adv Loss: 2.0642 | Acc: 90.62% | Adv Acc: 53.12%\n",
      "Epoch: 3 [6400/60000] Loss: 0.1792 | Adv Loss: 2.0820 | Acc: 91.99% | Adv Acc: 56.45%\n",
      "Epoch: 3 [12800/60000] Loss: 0.1708 | Adv Loss: 1.1940 | Acc: 91.64% | Adv Acc: 56.34%\n",
      "Epoch: 3 [19200/60000] Loss: 0.1296 | Adv Loss: 1.8028 | Acc: 91.69% | Adv Acc: 56.29%\n",
      "Epoch: 3 [25600/60000] Loss: 0.2624 | Adv Loss: 2.2085 | Acc: 91.75% | Adv Acc: 56.20%\n",
      "Epoch: 3 [32000/60000] Loss: 0.1350 | Adv Loss: 1.7220 | Acc: 91.92% | Adv Acc: 56.25%\n",
      "Epoch: 3 [38400/60000] Loss: 0.1030 | Adv Loss: 1.8077 | Acc: 92.04% | Adv Acc: 56.48%\n",
      "Epoch: 3 [44800/60000] Loss: 0.3491 | Adv Loss: 2.9600 | Acc: 91.98% | Adv Acc: 55.84%\n",
      "Epoch: 3 [51200/60000] Loss: 0.0967 | Adv Loss: 1.8718 | Acc: 92.07% | Adv Acc: 56.05%\n",
      "Epoch: 3 [57600/60000] Loss: 0.2466 | Adv Loss: 2.3872 | Acc: 92.10% | Adv Acc: 55.94%\n",
      "==> Epoch: 3 | Avg Loss: 0.2224 | Avg Adv Loss: 2.1982 | Avg Acc: 92.09% | Avg Adv Acc: 56.03%\n",
      "Epoch 3/5 completed\n",
      "\n",
      "Epoch: 4 [0/60000] Loss: 0.1758 | Adv Loss: 2.3020 | Acc: 93.75% | Adv Acc: 48.44%\n",
      "Epoch: 4 [6400/60000] Loss: 0.1028 | Adv Loss: 1.3450 | Acc: 91.79% | Adv Acc: 54.70%\n",
      "Epoch: 4 [12800/60000] Loss: 0.2123 | Adv Loss: 2.5301 | Acc: 92.63% | Adv Acc: 55.99%\n",
      "Epoch: 4 [19200/60000] Loss: 0.2267 | Adv Loss: 2.5164 | Acc: 92.02% | Adv Acc: 54.72%\n",
      "Epoch: 4 [25600/60000] Loss: 0.2579 | Adv Loss: 3.7901 | Acc: 91.28% | Adv Acc: 53.13%\n",
      "Epoch: 4 [32000/60000] Loss: 0.1210 | Adv Loss: 2.9712 | Acc: 91.28% | Adv Acc: 52.93%\n",
      "Epoch: 4 [38400/60000] Loss: 0.2790 | Adv Loss: 2.9333 | Acc: 91.29% | Adv Acc: 53.16%\n",
      "Epoch: 4 [44800/60000] Loss: 0.2016 | Adv Loss: 2.5142 | Acc: 91.41% | Adv Acc: 53.34%\n",
      "Epoch: 4 [51200/60000] Loss: 0.2958 | Adv Loss: 3.0733 | Acc: 91.57% | Adv Acc: 53.89%\n",
      "Epoch: 4 [57600/60000] Loss: 0.2139 | Adv Loss: 3.1900 | Acc: 91.54% | Adv Acc: 53.75%\n",
      "==> Epoch: 4 | Avg Loss: 0.2403 | Avg Adv Loss: 2.6711 | Avg Acc: 91.60% | Avg Adv Acc: 53.72%\n",
      "Epoch 4/5 completed\n",
      "\n",
      "Epoch: 5 [0/60000] Loss: 0.2334 | Adv Loss: 2.3474 | Acc: 92.19% | Adv Acc: 56.25%\n",
      "Epoch: 5 [6400/60000] Loss: 0.1407 | Adv Loss: 2.3034 | Acc: 93.60% | Adv Acc: 56.40%\n",
      "Epoch: 5 [12800/60000] Loss: 0.1628 | Adv Loss: 2.3223 | Acc: 93.60% | Adv Acc: 56.72%\n",
      "Epoch: 5 [19200/60000] Loss: 0.1999 | Adv Loss: 3.2879 | Acc: 93.49% | Adv Acc: 56.04%\n",
      "Epoch: 5 [25600/60000] Loss: 0.1664 | Adv Loss: 2.1235 | Acc: 93.33% | Adv Acc: 56.14%\n",
      "Epoch: 5 [32000/60000] Loss: 0.0761 | Adv Loss: 2.9498 | Acc: 93.46% | Adv Acc: 56.07%\n",
      "Epoch: 5 [38400/60000] Loss: 0.2490 | Adv Loss: 2.6176 | Acc: 93.44% | Adv Acc: 55.82%\n",
      "Epoch: 5 [44800/60000] Loss: 0.2137 | Adv Loss: 2.3154 | Acc: 93.34% | Adv Acc: 55.32%\n",
      "Epoch: 5 [51200/60000] Loss: 0.0999 | Adv Loss: 2.1261 | Acc: 93.44% | Adv Acc: 55.53%\n",
      "Epoch: 5 [57600/60000] Loss: 0.1333 | Adv Loss: 2.3460 | Acc: 93.40% | Adv Acc: 55.54%\n",
      "==> Epoch: 5 | Avg Loss: 0.1827 | Avg Adv Loss: 2.5286 | Avg Acc: 93.40% | Avg Adv Acc: 55.58%\n",
      "Epoch 5/5 completed\n",
      "\n",
      "Adversarial training completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet50().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "epsilon = 0.1  \n",
    "for epoch in range(epochs):\n",
    "    train(model, device, train_loader, optimizer, epsilon, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed\\n\")\n",
    "\n",
    "print(\"Adversarial training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3759241-2b47-49ca-a6d0-e252aa4273c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
